{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b064ee-841b-4ff7-8a34-a236b87bff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandr_ermolenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandr_ermolenko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2025-01-09 18:38:36,872 : INFO : Loading dictionaries from /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pymorphy3_dicts_ru/data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество текстов: 10000\n",
      "Обучающая выборка: 8000\n",
      "Тестовая выборка: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 18:38:36,899 : INFO : format: 2.4, revision: 417150, updated: 2022-01-08T22:09:24.565962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработка текстов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 18:45:59,136 : INFO : collecting all words and their counts\n",
      "2025-01-09 18:45:59,137 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-01-09 18:45:59,289 : INFO : collected 69393 word types from a corpus of 1115763 raw words and 8000 sentences\n",
      "2025-01-09 18:45:59,289 : INFO : Creating a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение модели Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 18:45:59,341 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 34983 unique words (50.41% of original 69393, drops 34410)', 'datetime': '2025-01-09T18:45:59.340909', 'gensim': '4.3.3', 'python': '3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.1.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-01-09 18:45:59,341 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1081353 word corpus (96.92% of original 1115763, drops 34410)', 'datetime': '2025-01-09T18:45:59.341575', 'gensim': '4.3.3', 'python': '3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.1.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-01-09 18:45:59,411 : INFO : deleting the raw counts dictionary of 69393 items\n",
      "2025-01-09 18:45:59,413 : INFO : sample=0.001 downsamples 17 most-common words\n",
      "2025-01-09 18:45:59,413 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1054320.4308552009 word corpus (97.5%% of prior 1081353)', 'datetime': '2025-01-09T18:45:59.413531', 'gensim': '4.3.3', 'python': '3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.1.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-01-09 18:45:59,526 : INFO : estimated required memory for 34983 words and 100 dimensions: 45477900 bytes\n",
      "2025-01-09 18:45:59,526 : INFO : resetting layer weights\n",
      "2025-01-09 18:45:59,538 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-01-09T18:45:59.538723', 'gensim': '4.3.3', 'python': '3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.1.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2025-01-09 18:45:59,539 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 34983 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-01-09T18:45:59.539307', 'gensim': '4.3.3', 'python': '3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.1.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-01-09 18:45:59,931 : INFO : EPOCH 0: training on 1115763 raw words (1054153 effective words) took 0.4s, 2702848 effective words/s\n",
      "2025-01-09 18:46:00,313 : INFO : EPOCH 1: training on 1115763 raw words (1054169 effective words) took 0.4s, 2772010 effective words/s\n",
      "2025-01-09 18:46:00,694 : INFO : EPOCH 2: training on 1115763 raw words (1054501 effective words) took 0.4s, 2773920 effective words/s\n",
      "2025-01-09 18:46:01,074 : INFO : EPOCH 3: training on 1115763 raw words (1054167 effective words) took 0.4s, 2785054 effective words/s\n",
      "2025-01-09 18:46:01,456 : INFO : EPOCH 4: training on 1115763 raw words (1054106 effective words) took 0.4s, 2770410 effective words/s\n",
      "2025-01-09 18:46:01,835 : INFO : EPOCH 5: training on 1115763 raw words (1053883 effective words) took 0.4s, 2787246 effective words/s\n",
      "2025-01-09 18:46:02,214 : INFO : EPOCH 6: training on 1115763 raw words (1054478 effective words) took 0.4s, 2801484 effective words/s\n",
      "2025-01-09 18:46:02,594 : INFO : EPOCH 7: training on 1115763 raw words (1054431 effective words) took 0.4s, 2784571 effective words/s\n",
      "2025-01-09 18:46:02,978 : INFO : EPOCH 8: training on 1115763 raw words (1054551 effective words) took 0.4s, 2758312 effective words/s\n",
      "2025-01-09 18:46:03,359 : INFO : EPOCH 9: training on 1115763 raw words (1054238 effective words) took 0.4s, 2778764 effective words/s\n",
      "2025-01-09 18:46:03,359 : INFO : Word2Vec lifecycle event {'msg': 'training on 11157630 raw words (10542677 effective words) took 3.8s, 2759825 effective words/s', 'datetime': '2025-01-09T18:46:03.359582', 'gensim': '4.3.3', 'python': '3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.1.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-01-09 18:46:03,359 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=34983, vector_size=100, alpha=0.025>', 'datetime': '2025-01-09T18:46:03.359789', 'gensim': '4.3.3', 'python': '3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.1.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание векторов документов (усреднение)...\n",
      "Обучение модели SVM (усреднение)...\n",
      "Оценка модели SVM (усреднение)...\n",
      "Точность модели SVM: 0.83\n",
      "Отчет о классификации:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.60      0.43      0.50        72\n",
      "     culture       0.87      0.91      0.89       279\n",
      "   economics       0.79      0.86      0.82       275\n",
      "      forces       0.77      0.81      0.79       154\n",
      "        life       0.78      0.81      0.79       273\n",
      "       media       0.81      0.76      0.78       295\n",
      "     science       0.83      0.83      0.83       286\n",
      "       sport       0.97      0.96      0.97       288\n",
      "       style       0.79      0.69      0.74        39\n",
      "      travel       0.78      0.54      0.64        39\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.80      0.76      0.77      2000\n",
      "weighted avg       0.82      0.83      0.82      2000\n",
      "\n",
      "Создание взвешенных векторов документов с использованием TF-IDF...\n",
      "Создание взвешенных векторов документов...\n",
      "Обучение модели SVM (взвешенное усреднение)...\n",
      "Оценка модели SVM (взвешенное усреднение)...\n",
      "Точность модели SVM с TF-IDF взвешиванием: 0.83\n",
      "Отчет о классификации (взвешенное усреднение):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.65      0.31      0.42        72\n",
      "     culture       0.88      0.92      0.90       279\n",
      "   economics       0.78      0.90      0.84       275\n",
      "      forces       0.76      0.81      0.78       154\n",
      "        life       0.78      0.81      0.80       273\n",
      "       media       0.82      0.77      0.79       295\n",
      "     science       0.84      0.82      0.83       286\n",
      "       sport       0.97      0.97      0.97       288\n",
      "       style       0.78      0.74      0.76        39\n",
      "      travel       0.79      0.56      0.66        39\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.80      0.76      0.77      2000\n",
      "weighted avg       0.83      0.83      0.83      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Iterator\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import logging\n",
    "\n",
    "# Настройка логирования (опционально)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Класс для представления текста с меткой\n",
    "class Text:\n",
    "    def __init__(self, label: str, text: str):\n",
    "        self.label = label\n",
    "        self.text = text\n",
    "\n",
    "# Функция для чтения текстов из файла\n",
    "def read_texts(fn: str) -> Iterator[Text]:\n",
    "    with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            parts = line.strip().split(\"\\t\", 1)  # Разбиваем только по первой табуляции\n",
    "            if len(parts) != 2:\n",
    "                logging.warning(f\"Строка {line_number}: некорректный формат\")\n",
    "                continue\n",
    "            try:\n",
    "                yield Text(*parts)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Строка {line_number}: ошибка при создании объекта Text - {e}\")\n",
    "\n",
    "# Предобработка текста: токенизация, удаление стоп-слов, лемматизация с использованием spaCy\n",
    "def preprocess_spacy(text: str, stop_words: set, nlp) -> list:\n",
    "    \"\"\"\n",
    "    Предобработка текста с использованием spaCy:\n",
    "    - Приведение к нижнему регистру\n",
    "    - Токенизация\n",
    "    - Удаление пунктуации и стоп-слов\n",
    "    - Лемматизация\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.is_alpha and token.lemma_ not in stop_words\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "def main():\n",
    "    # Шаг 1: Чтение данных\n",
    "    texts = list(read_texts('../../data/news.txt'))\n",
    "    \n",
    "    # Инициализация списков для меток и текстов\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for item in texts:\n",
    "        label, text = item.label, item.text\n",
    "        X.append(text)\n",
    "        y.append(label)\n",
    "    \n",
    "    print(f\"Общее количество текстов: {len(texts)}\")\n",
    "    \n",
    "    # Шаг 2: Разделение данных на обучающую и тестовую выборки\n",
    "    X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Обучающая выборка: {len(X_train_texts)}\")\n",
    "    print(f\"Тестовая выборка: {len(X_test_texts)}\")\n",
    "    \n",
    "    # Шаг 3: Предобработка текстов\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    nlp = spacy.load(\"ru_core_news_sm\")\n",
    "    \n",
    "    print(\"Предобработка текстов...\")\n",
    "    X_train_tokens = [preprocess_spacy(text, stop_words, nlp) for text in X_train_texts]\n",
    "    X_test_tokens = [preprocess_spacy(text, stop_words, nlp) for text in X_test_texts]\n",
    "    \n",
    "    # Шаг 4: Обучение модели Word2Vec\n",
    "    print(\"Обучение модели Word2Vec...\")\n",
    "    vector_size = 100\n",
    "    window = 5\n",
    "    min_count = 2\n",
    "    workers = 4\n",
    "    \n",
    "    model = Word2Vec(\n",
    "        sentences=X_train_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        epochs=10  # Количество эпох обучения\n",
    "    )\n",
    "    \n",
    "    # Сохранение модели (опционально)\n",
    "    # model.save(\"word2vec.model\")\n",
    "    \n",
    "    # Шаг 5: Представление документов через усреднение векторов слов\n",
    "    def document_vector(doc, model):\n",
    "        # Исключение слов, отсутствующих в модели\n",
    "        doc = [word for word in doc if word in model.wv.key_to_index]\n",
    "        if len(doc) == 0:\n",
    "            return np.zeros(model.vector_size)\n",
    "        return np.mean(model.wv[doc], axis=0)\n",
    "    \n",
    "    print(\"Создание векторов документов (усреднение)...\")\n",
    "    X_train_vec = np.array([document_vector(doc, model) for doc in X_train_tokens])\n",
    "    X_test_vec = np.array([document_vector(doc, model) for doc in X_test_tokens])\n",
    "    \n",
    "    # Шаг 6: Классификация текстов с использованием SVM\n",
    "    print(\"Обучение модели SVM (усреднение)...\")\n",
    "    svm = SVC(kernel='linear', random_state=42)\n",
    "    svm.fit(X_train_vec, y_train)\n",
    "    \n",
    "    print(\"Оценка модели SVM (усреднение)...\")\n",
    "    y_pred = svm.predict(X_test_vec)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Точность модели SVM: {accuracy:.2f}\")\n",
    "    print(\"Отчет о классификации:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Шаг 7: Альтернативный способ представления документов (взвешенное усреднение с TF-IDF)\n",
    "    print(\"Создание взвешенных векторов документов с использованием TF-IDF...\")\n",
    "    \n",
    "    # Объединение токенов обратно в строки для TfidfVectorizer\n",
    "    X_train_processed = [' '.join(doc) for doc in X_train_tokens]\n",
    "    X_test_processed = [' '.join(doc) for doc in X_test_tokens]\n",
    "    \n",
    "    # Создание TF-IDF векторизатора\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(X_train_processed)\n",
    "    \n",
    "    # Получение словаря TF-IDF\n",
    "    tfidf_vocab = tfidf.vocabulary_\n",
    "    idf = tfidf.idf_\n",
    "    \n",
    "    # Создание словаря IDF для быстрого доступа\n",
    "    idf_dict = dict(zip(tfidf.get_feature_names_out(), idf))\n",
    "    \n",
    "    def weighted_document_vector(doc, model, idf_dict):\n",
    "        vectors = []\n",
    "        weights = []\n",
    "        for word in doc:\n",
    "            if word in model.wv.key_to_index and word in idf_dict:\n",
    "                vectors.append(model.wv[word])\n",
    "                weights.append(idf_dict[word])\n",
    "        if not vectors:\n",
    "            return np.zeros(model.vector_size)\n",
    "        vectors = np.array(vectors)\n",
    "        weights = np.array(weights)\n",
    "        weighted_avg = np.average(vectors, axis=0, weights=weights)\n",
    "        return weighted_avg\n",
    "    \n",
    "    print(\"Создание взвешенных векторов документов...\")\n",
    "    X_train_weighted = np.array([weighted_document_vector(doc, model, idf_dict) for doc in X_train_tokens])\n",
    "    X_test_weighted = np.array([weighted_document_vector(doc, model, idf_dict) for doc in X_test_tokens])\n",
    "    \n",
    "    # Шаг 8: Классификация текстов с использованием SVM на взвешенных векторах\n",
    "    print(\"Обучение модели SVM (взвешенное усреднение)...\")\n",
    "    svm_weighted = SVC(kernel='linear', random_state=42)\n",
    "    svm_weighted.fit(X_train_weighted, y_train)\n",
    "    \n",
    "    print(\"Оценка модели SVM (взвешенное усреднение)...\")\n",
    "    y_pred_weighted = svm_weighted.predict(X_test_weighted)\n",
    "    accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
    "    print(f\"Точность модели SVM с TF-IDF взвешиванием: {accuracy_weighted:.2f}\")\n",
    "    print(\"Отчет о классификации (взвешенное усреднение):\")\n",
    "    print(classification_report(y_test, y_pred_weighted))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df296a6f-8cb9-4b11-9291-581c6da9ead6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
